\documentclass[preview]{standalone}
%\usepackage{prelude}
\input{prelude}


\begin{document}

\section{Preliminaries} \label{ch:prelim}

%\subsection{Transition Systems}
\viewsNC will be defined on \chgphsN. Instead of directly providing the definition of \achgphN we will consider less powerful classes of models to represent systems that are extended by \chgphsN.

Firstly we will consider transition systems. Transition systems are basically digraphs consisting of \emph{states} and \emph{transitions} in place of nodes and edges. A state describes some information about a system at a certain moment of its behavior. Considering a traffic light displaying green could be considered as one state whereas displaying red could be another one. Transitions model the progression of the system from one state to another one. Sticking to the example of the traffic light a transition could model the switch from state green light being displayed to the state of red light being displayed. There are several variants of transitions systems. We will use transition systems with \emph{action names} and \emph{atomic propositions} for states as in \redcomment{BAIER}. Actions are used for communication between processes. We denote them with Greek letters ($\alpha, \beta, \gamma, \dots$). Atomic propositions are simple facts about states. For instance "x is greater than 20" or "red and yellow light are on" could be atomic propositions. They are assigned to a state by a labeling function \labelingfct.

\begin{definition}[\!{\cite{Baier2008}, Definition 2.1.}]
	A \emph{transition system} TS is a tuple \transitionsystem where
	\begin{itemize}
		\item \states is a set of states,
		\item \actions is a set of actions,
		\item $\transitionrel \subseteq \states \times \actions \times \states$ is transition relation,
		\item $\initstates \subseteq \states$ is a set of initial states,
		\item \atomicprops is a set of atomic propositions, and
		\item $\labelingfct : \states \to \powerset{\atomicprops}$
	\end{itemize}
\end{definition}

A transition system is called \emph{finite} if \states, \atomicprops and \labelingfct are finite. The intuitive behavior of transition systems is as follows. The evolution of a transition system starts in some state $\state \in \initstates$. If the set of \initstates of initial states is empty the transition system has no behavior at all. From the initial state the transition system evolves according to the transition relation \transitionrel. The evolution ends in a state that has no outgoing transitions. For every state there may be several possible transitions to be taken. The choice of which one is take is done nondeterministically. That is the outcome of the selection can not be know a priori. It is especially not following any probability distribution. Hence there can not be made any statement about the likelihood of a transition being selected.

In contrast with Markov Chains this nondeterministic behavior is replaced with a probabilistic one. That is for every state there exists a probability distribution that describes the chance of a transition of being selected. It is important to note that Markov Chains are memoryless in the sense that the system evolution is not dependent on the history of only on the current state. That is the evolution of the system does not depend on the sequence of so far traversed states with the transitions. That the nondeterministic behavior is replaced with a probabilistic one also means that there are no actions in Markov chains.
%\redcomment{NOTES BEGIN}
%\begin{itemize}
%	\item Markov Chain (MC)
%	\item transition systems to markov chains: nondeterministic choices replaced by probablistic
%	\item successor chosen according to probability distribution
%	\item distribution only dependent on current state \state (not path)
%	\item system evolution not dependent on history but only current state $\to$ \emph{memoryless property}
%\end{itemize}
%
%\redcomment{NOTES END}

\begin{definition}[\!{\cite{Baier2008}, Definition 10.1.}]
	A \emph{(discrete-time) Markov chain} is a tuple $\autm = (\states, \probtfunc, \initdistrib, \atomicprops, \labelingfct)$ where 
	\begin{itemize}
		\item \states is a countable, nonempty set of states,
		\item $\probtfunc : \states \times \states \to [0,1]$ is the \emph{transition probability function}, such that for all states \state:
		\begin{center}
			$\bbigsum{\state' \in \states}\probtfunc(\state,\states') = 1$.	
		\end{center}
		\item $\initdistrib : \states \to [0,1]$ is the \emph{initial distribution}, such that $\mathlarger{\sum_{\state \in \states}} \initdistrib(\state) = 1$, and
		\item \atomicprops is a set of atomic propositions and,
		\item $\labelingfct : \states \to \powerset{\atomicprops}$ a labeling function.		
	\end{itemize}
\end{definition}


A Markov Chain Process \autm is called \emph{finite} if \states and \atomicprops are finite. \redcomment{For finite \autm, the \emph{size} of \autm, denoted \emph{size}(\autm),  is the number of states plus the number of pairs $(\state, \state') \in \states \times \states$ with $\probtfunc(\state, \state') > 0.$ OMIT??}
The probability function \probtfunc specifies for each state \state the probability $\probtfunc(\state,\state')$ of moving from \state to \state' in one step. The constraint put on \probtfunc in the second item ensures that \probtfunc is a probability distribution. The value $\initdistrib(\state)$ specifies the likelihood that the system evolution starts in \state. All states \state with $\initdistrib(\state) > 0$ are  considered \emph{initial states}. States \state' with $\probtfunc(\state, \state') > 0$ are viewed as possible successors of the state \state. The operational behavior is as follows. A initial state $\state_0$ with $\initdistrib(\state_0) > 0$ is yielded. Afterwards in each state a transition is yielded at random according to the probability distribution \probtfunc in that state. \redcomment{The evolution of a Markov chain ends in a state \state \iffN $\probtfunc(\state, \state) = 1$}
%\begin{itemize}
%	\redcomment{\item has no actions \\ "As compositional approaches for Markov models are outside the scope of this monograph,
%	actions are irrelevant in this chapter and are therefore omitted."}
%\end{itemize}

%\redcomment{NOTES BEGIN}
%\begin{itemize}
%	\item Probability Function \probtfunc specifies for each state \state the probability \probtfunc(\state,\state') of moving from \state to \state' in one step.
%	\item constraint on \probtfunc ensures that \probtfunc is distribution
%	\item \initdistrib(\state) specifies system evolution starts in \state
%	\item states \state with $\initdistrib(\state) > 0$ are  considered \emph{initial states}
%	\item states \state' with $\probtfunc(\state, \state') > 0$ are view as possible successors of \state
%	\item has no actions \\ "As compositional approaches for Markov models are outside the scope of this monograph,
%	actions are irrelevant in this chapter and are therefore omitted."
%\end{itemize}
%
%\redcomment{NOTES END}


%\subsection{Markov Decision Process} 

%\redcomment{NOTES BEGIN}
\redcomment{Disadvantage of Markov Chain}

An \mdpN permits both probabilistic and nondeterministic choices thereby is a model that somewhat merges the concept of transition systems with the concept of Markov chains. They add probabilistic choices to transition systems or add nondeterminism to Markov chains. Probabilistic choices may be used to model the (probabilistic) behavior of the environment. But to do so statistical experiments are required to obtain adequate distributions that model the average behavior of the environment. If this information is not available too hard to obtain, or a guarantee about system properties is required nondeterminism is the natural option for modeling in these cases. Further usecases of \mdpsN are randomized distributed algorithms and abstraction in Markov chains. Randomized distributed algorithms are nondeterministic because there is a nondeterministic choice which process performs the next step and randomized because they have rather restricted set of actions that have a random nature. Abstraction in Markov chains can be feasible if states have been grouped by \atomicprops and and have a wide range of transition probabilities. If that is the case selection of a transition is rather non deterministic and thereby can be abstracted with an \mdpN by replacing the probabilities with nondeterminism.
%\begin{itemize}
%	\item probabilistic choices: possible outcomes for of randomized actions -> requires statistical experiments to obtain adequate distributions that model average behavior of the environment
%	\item information not available or guarantee about system properties is required -> nondeterminism
%	\item Another example: randomized distributed algorithms. Non-determinism: interleaving behavior: nondeterministic choice which process, probabilistic: have rather restricted set of actions that have a random nature
%	\item used for abstraction in markov chains: states grouped by \atomicprops and have a wide range of transition probabilities -> essentially nondeterminism -> transition probabilities are replaced by nondeterminism		
%\end{itemize}
%
%\redcomment{NOTES END}
%\redcomment{NOTES BEGIN}
%
%\begin{itemize}
%	\item Markov decision process (MDP) 
%	\item idea: Adding nondeterminism to markov chains. MDPs permit both probabilistic and nondeterministic choices 
%	\item probabilistic choices: possible outcomes for of randomized actions -> requires statistical experiments to obtain adequate distributions that model average behavior of the environment
%	\item information not available or guarantee about system properties is required -> nondeterminism
%	\item Another example: randomized distributed algorithms. Non-determinism: interleaving behavior: nondeterministic choice which process, probabilistic: have rather restricted set of actions that have a random nature
%	\item used for abstraction in markov chains: states grouped by \atomicprops and have a wide range of transition probabilities -> essentially nondeterminism -> transition probabilities are replaced by nondeterminism		
%\end{itemize}

%\redcomment{NOTES END}

\begin{definition}[\!{\cite{Baier2008}, Definition 10.81.}]
	A \emph{Markov decision process} (\mdpN) is a tuple $\mdp = \mdptuple$ where
	\begin{itemize}
		\item \states is a countable set of states,
		\item \actions is a set of actions,
		\item $\probtfunc : \states \times \actions \times \states \to [0,1]$ is the transition probability function such that for all states $\state \in \states$ and actions $\action \in \actions$:
		\begin{center}
			$\bbigsum{\state' \in \states}\probtfunc(\state,\action,\state') \in \{0,1\}$,
		\end{center}
		\item $\initdistrib : \states \to [0,1]$ is the initial distribution such that $\sum_{\state \in \states} \initdistrib(\state) = 1$,
		\item \atomicprops is  a set of atomic propositions and
		\item $\labelingfct : \states \to \powerset{\atomicprops}$ a labeling function.
	\end{itemize}
	An action \action is \emph{enabled} in state \state if and only if $\sum_{\state' \in \states} \probtfunc(\state,\action,\state') = 1$. Let $\actions(\state)$ denote the set of enabled actions in \state. For any state $\state \in \states$, it is required that $\actions(\state) \neq \emptyset$. Each state \state' for which $\probtfunc(\state,\action,\state') > 0$ is called an \emph{\action-successor} of \state.
\end{definition}

An \mdpN is called \emph{finite} if \states, \actions and \atomicprops are finite.The transition probabilities $\probtfunc(\state, \action, t)$ can be arbitrary real numbers in $[0,1]$ (that sum up to 1 or 0 for fixed \state and \action). For algorithmic purposes they are assumed to be rational. The unique initial distribution \initdistrib could be generalized to set of \initdistrib with nondeterministic choice at the beginning. For sake of simplicity there is just one single distribution. The operational behavior is as follows. A starting state $\state_0$ yielded by \initdistrib with $\initdistrib(\state_0) > 0$. From there on nondeterministic choice of enabled action takes place followed by a probabilistic choice of a state. The action fixed in the step of nondeterministic selection. Any Markov chain is an \mdpN in which for every state \state, $\actions(\state)$ is a singleton set. Conversely an \mdpN with the property of $\actions(\state) = 1$ is a Markov chain. Thus Markov chains are a proper subset of Markov decision processes.

For convenience for $\state_1, \state_2 \in \states$ and $\action \in \actions$ we will write $(\state_1, \action, \state_2) \in \probtfunc$ \iffN $\probtfunc(\state_1, \action, \state_2) > 0$, that is to say if there is a non-zero probability of evolving from state $\state_1$ to state $\state_2$ with action \action. Analogously we will write $\state \in \initdistrib$ \iffN is an initial state ($\initdistrib(\state) > 0$).

%\redcomment{not sure if I should: $I := \initdistrib$ thereby meaning the underlying set}
\redcomment{Basic Prism}

\begin{figure}[h]
	\centering \input{./02/images/exampleMdp}
	\caption{Simplified representation of \mdp (left) and the \viewN \viewinitstates on it(left)}
	\label{fig:exampleMdp}  
\end{figure}

%\redcomment{NOTES BEGIN}
%
%\begin{itemize}
%	\item $\probtfunc(\state, \action, t)$ can be arbitrary real numbers in $[0,1]$ (sum up to 1 or 0 for fixed \state and \action), for algorithmic purposes rational
%	\item unique initial distribution \initdistrib. Could be generalized to set of \initdistrib with nondeterministic choice at the beginning. For sake of simplicity: one single distribution
%	\item operational behavior:
%	\begin{itemize}
%		\item starting state $\state_0$ yielded by \initdistrib with $\initdistrib(\state_0) > 0$
%		\item nondeterministic choice of enabled action (i.e. Probability sums up to one)
%		\item probabilistic choice of state (action fixed by nondeterministic selection)
%	\end{itemize}
%	
%	\item $MC = MDP \iff \forall \state \in \states : |\actions(\state)| = 1$
%	
%	\item $\implies$ MCs are a proper subset of MDPs
%	
%\end{itemize}
%
%\redcomment{NOTES END}
%
%\redcomment{CONVENTION BEGIN}



\end{document}
 
