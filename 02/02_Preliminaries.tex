\documentclass[preview]{standalone}
%\usepackage{prelude}
\input{prelude}


\begin{document}

\section{Preliminaries} \label{ch:prelim}

%\subsection{Transition Systems}
\viewsNC will be defined on \chgphsN. Instead of providing the definition of \achgphN directly, we will consider less powerful classes of models to represent systems extended by \chgphsN.

First, we will consider \emph{transition systems}. Transition systems are basically digraphs consisting of \emph{states} and \emph{transitions} in place of nodes and edges. A state describes some information about a system at a certain moment of its behavior, a transition the evolution from one state to another. We will use transition systems with \emph{action names} and \emph{atomic propositions} for states as in \cite{Baier2008}.

\begin{definition}[\!{\cite{Baier2008}, Definition 2.1.}]
	A \emph{transition system} TS is a tuple \\ \transitionsystem where
	\begin{itemize}
		\item \states is a set of states,
		\item \actions is a set of actions,
		\item $\transitionrel \subseteq \states \times \actions \times \states$ is transition relation,
		\item $\initstates \subseteq \states$ is a set of initial states,
		\item \atomicprops is a set of atomic propositions, and
		\item $\labelingfct : \states \to \powerset{\atomicprops}$
	\end{itemize}
\end{definition}

%Considering a traffic light displaying green could be considered as one state whereas displaying red could be another one. Transitions model the progression of the system from one state to another one. Sticking to the example of the traffic light a transition could model the switch from state green light being displayed to the state of red light being displayed. There are several variants of transitions systems. 



A transition system is called \emph{finite} if \states, \atomicprops, and \labelingfct are finite. Actions are used for communication between processes. We denote them with Greek letters ($\alpha, \beta, \gamma, \dots$). Atomic propositions are simple facts about states. For instance "x is greater than 20" or "red and yellow light are on" could be atomic propositions. We will denote atomic propositions with Arabic letters ($a,b,c,\dots$). They are assigned to a state by a labeling function \labelingfct.

The intuitive behavior of transition systems is as follows. The evolution of a transition system starts in some state $\state \in \initstates$. If the set of \initstates of initial states is empty, the transition system has no behavior at all. From the initial state the transition system evolves according to the transition relation \transitionrel. The evolution ends in a state that has no outgoing transitions. For each state there may be several possible transitions. The choice of which one to take is made nondeterministically. That is, the outcome of the selection can not be known a priori. In particular, it does not follow any probability distribution. Hence, no statement can be made about the likelihood of a transition being selected.

This is in contrast to \emph{Markov Chains}, where the nondeterministic behavior is replaced by a probabilistic one. That is, for each state there exists a probability distribution describing the chance of a transition being selected. There are no actions and no nondeterminism in Markov Chains.
%\redcomment{NOTES BEGIN}
%\begin{itemize}
%	\item Markov Chain (MC)
%	\item transition systems to markov chains: nondeterministic choices replaced by probablistic
%	\item successor chosen according to probability distribution
%	\item distribution only dependent on current state \state (not path)
%	\item system evolution not dependent on history but only current state $\to$ \emph{memoryless property}
%\end{itemize}
%
%\redcomment{NOTES END}
%\newpage
\begin{definition}[\!{\cite{Baier2008}, Definition 10.1.}]
	A \emph{(discrete-time) Markov Chain} is a tuple $\autm = (\states, \probtfunc, \initdistrib, \atomicprops, \labelingfct)$ where 
	\begin{itemize}
		\item \states is a countable, nonempty set of states,
		\item $\probtfunc : \states \times \states \to [0,1]$ is the \emph{transition probability function}, such that for all states \state:
		\begin{center}
			$\bbigsum{\state' \in \states}\probtfunc(\state,\states') = 1$.	
		\end{center}
		\item $\initdistrib : \states \to [0,1]$ is the \emph{initial distribution}, such that $\mathlarger{\sum_{\state \in \states}} \initdistrib(\state) = 1$, and
		\item \atomicprops is a set of atomic propositions and,
		\item $\labelingfct : \states \to \powerset{\atomicprops}$ a labeling function.		
	\end{itemize}
\end{definition}

A Markov Chain (MC) \autm is called \emph{finite} if \states and \atomicprops are finite. The probability function \probtfunc specifies for each state \state the probability $\probtfunc(\state,\state')$ of moving from \state to \state' in one step. The constraint put on \probtfunc in the second item ensures that \probtfunc is a probability distribution. The value $\initdistrib(\state)$ specifies the probability that the system evolution will start in \state. All states \state with $\initdistrib(\state) > 0$ are considered \emph{initial states}. States \state' with $\probtfunc(\state, \state') > 0$ are considered possible successors of state \state. The operational behavior is as follows. The initial distribution \initdistrib yields a state $\state_0$. Afterwards, in each state a transition is yielded at random according to the probability distribution \probtfunc in that state.

The disadvantage of Markov Chains is that they do not enable process intercommunication and do not permit nondeterminism, but only probabilistic evolution. \emph{Markov Decision Processes} (\mdpsN) allow both probabilistic and nondeterministic choices. An \mdpN is thus a model that somewhat merges the concept of transition systems with the concept of Markov Chains.

\begin{definition}[\!{\cite{Baier2008}, Definition 10.81.}]
	A \emph{Markov decision process} (\mdpN) is a tuple $\mdp = \mdptuple$ where
	\begin{itemize}
		\item \states is a countable set of states,
		\item \actions is a set of actions,
		\item $\probtfunc : \states \times \actions \times \states \to [0,1]$ is the transition probability function such that for all states $\state \in \states$ and actions $\action \in \actions$:
		\begin{center}
			$\bbigsum{\state' \in \states}\probtfunc(\state,\action,\state') \in \{0,1\}$,
		\end{center}
		\item $\initdistrib : \states \to [0,1]$ is the initial distribution such that $\sum_{\state \in \states} \initdistrib(\state) = 1$,
		\item \atomicprops is  a set of atomic propositions and
		\item $\labelingfct : \states \to \powerset{\atomicprops}$ a labeling function.
	\end{itemize}
	An action \action is \emph{enabled} in state \state if and only if $\sum_{\state' \in \states} \probtfunc(\state,\action,\state') = 1$. Let $\actions(\state)$ denote the set of enabled actions in \state. For any state $\state \in \states$, it is required that $\actions(\state) \neq \emptyset$. Each state \state' for which $\probtfunc(\state,\action,\state') > 0$ is called an \emph{\action-successor} of \state.
\end{definition}

An \mdpN is called \emph{finite} if \states, \actions and \atomicprops are finite. The transition probabilities $\probtfunc(\state, \action, t)$ can be arbitrary real numbers in $[0,1]$ (that sum up to 1 or 0 for fixed \state and \action). For algorithmic purposes, they are assumed to be rational.  The unique initial distribution \initdistrib could be generalized to a set of \initdistrib with a nondeterministic choice at the beginning. For simplicity, there is only one distribution. The operational behavior is as follows. A starting state $\state_0$ is yielded by \initdistrib with $\initdistrib(\state_0) > 0$. From there, first a nondeterministic choice of an enabled action takes place followed by a probabilistic choice of a state. The action is fixed with the step of nondeterministic choice. Every Markov Chain is an \mdpN in which for every state \state, $\actions(\state)$ is a singleton set. Conversely an \mdpN with the property of $\outacts(\state) = 1$ is a Markov Chain. Thus Markov Chains are a proper subset of Markov decision processes.

For convenience, we write $(\state_1, \action, \state_2) \in \probtfunc$ if $\probtfunc(\state_1, \action, \state_2) > 0$, where $\state_1, \state_2 \in \states$ and $\action \in \actions$. That is, we write $(\state_1, \action, \state_2) \in \probtfunc$ if there is a non-zero probability of evolving from state $\state_1$ to state $\state_2$ with action \action.
We define $\outacts(\state) := \{\action \mid (\state,\action,\smstate) \in \trans\}$ for $\state \in \states$. For $\state \in \states$, we call an element of $\outacts(\state)$ \emph{outgoing action} of \state. We say that a state \state "has an outgoing action \action" or "\action is outgoing from \state" if $\action \in \outacts(\state)$. We use analogous definition and terminology for incoming actions \inacts. 

\begin{exmp}
In Figure \ref{fig:exampleMdp} we see an graphical representation of an \mdpN. For this \mdpN it is $\states = \{\state_1, \state_2, \state_3, \state_4, \state_5, \state_6\}$, $\actions = \{\action, \actionb, \actionc\}$, $\atomicprops = \{a,b\}$. In Table \ref{tab:atomicpropsandlabelingfunction} we see the values for \labelingfct, \initdistrib and \trans.

%\begin{figure}
\begin{table}[h!]
	\parbox{.45\linewidth}{
		\begin{center}
%			
			\begin{tabular}{c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
				State & $\labelingfct(\state_i)$ & $\initdistrib(\state_i)$\\		
				\hline
				$\state_1$ & $\emptyset$ & $0.7$\\
				$\state_2$ & $\{a\}$ & $0$\\
				$\state_3$ & $\{a,b\}$ & $0.2$\\
				$\state_4$ & $\{b\}$ & $0.1$\\
				$\state_5$ & $\{a\}$ & $0$\\
				$\state_6$ & $\{b\}$ & $0$\\			
			\end{tabular}
		\end{center}
}
%\hspace{10mm}
\parbox{.45\linewidth}{
		\begin{center}
%			\caption{Your first table.}
%			\label{tab:trans}
			\begin{tabular}{c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
				$t \in \states \times \actions \times \states$ & $\trans(t)$\\		
				\hline
				$(\state_1,\action, \state_2)$ & $0.5$\\
				$(\state_1,\action, \state_4)$ & $1$\\
				$(\state_2,\actionb, \state_2)$ & $0.6$\\
				$(\state_2,\actionb, \state_6)$ & $0.4$\\
				$(\state_2,\actionc, \state_1)$ & $1$\\
				$(\state_3,\actionb, \state_3)$ & $0.4$\\
				$(\state_3,\actionb, \state_2)$ & $0.6$\\
				$(\state_3,\actionc, \state_1)$ & $0.8$\\
				$(\state_3,\actionc, \state_4)$ & $0.2$\\
				$(\state_4,\action, \state_4)$ & $0.5$\\
				$(\state_4,\actionc, \state_4)$ & $1$\\
				$(\state_4,\action, \state_3)$ & $0.5$\\
				$(\state_5,\actionb, \state_5)$ & $1$\\
				$(\state_6,\action, \state_6)$ & $1$\\				
			\end{tabular}
		\end{center}
	}
	\caption{Labeling Function, initial distribution (left) and transition probability function (right) of the \mdpN in Figure \ref{fig:exampleMdp}. Omitted values of $\trans(t)$ are meant to be zero.}
	\label{tab:atomicpropsandlabelingfunction}
\end{table}

%\end{figure}
We will declare \mdpsN by only providing its graphical representation. For this the sets $\states, \actions, \atomicprops$ are assumed to be minimal. That is, they contain no more elements than displayed in the graphical representation. Mostly we will use simplified graphical representations of \mdpsN. In these we will omit information. If actions are omitted, it is assumed that each transition $t \in \trans$ has a distinct action. If probabilities are omitted for each state it is assumed the uniform distribution on each set of outgoing transitions with the same action. If the initial distribution is omitted, it is assumed to be the uniform distribution. If atomic propositions are omitted it is assumed that the set of atomic propositions is empty and every state is mapped to the empty set by the labeling function. The purpose of simplified representations is to focus on and only show relevant information. The remaining information is considered irrelevant in these cases.

\begin{figure}[!htb]
	\centering \input{./02/images/exampleMdp}
	\caption{Simplified representation of \mdp (left) and the \viewN \viewinitstates on it(left)}
	\label{fig:exampleMdp}  
\end{figure}
\end{exmp}

In the implementation and evaluation chapters \ref{ch:viewimpl} and \ref{ch:eval} we will refer to \prism (PRobabilistIc Symbolic Model checker), which is why we will give a brief introduction on it. \prism is a model checker that can be used to define models and check them in an automated manner. In \prism, a model is defined with modules that interact with each other. A module consists of variables and commands. The (current) values of all variables in the module define the (current) local state of the module. The global state of the model is defined by the local state of all modules. A command in a module is of the form 
\[
\text{\texttt{[action] guard -> p\_1\!\;:\!\;update\_1\space+\space\dots\;\,+\space p\_m\!\;:\!\;update\_m;}} 
\]
where \texttt{p\_1, \dots\;\,, p\_m > 0} and \texttt{p\_1 + \dots\;\,+ p\_m = 1}.
The \texttt{guard} is a predicate on all variables of the model (including those of other modules). It may include operators such as negation (\texttt{!}), conjunction (\texttt{\&}) disjunction (\texttt{|}), arithmetic operators (\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}) or relational operators (\texttt{<}, \texttt{<=}, \texttt{>=}, \texttt{>}, \texttt{!=}, \texttt{=}, \texttt{=>}, \texttt{<=>}) as well as predefined functions.
An \texttt{update} represents a transition in the model, which normally reflects a change of state. As a state is defined by the values of all the variables, an update is specified by the assignment of new values to the variables of the module, possibly as a function of variables from other modules. The value of a variable remains unchanged, if it is not assigned a new value in an update. An update could look as follows:
\[
\text{\texttt{(x\_1'=1) \& (x\_2'=true) \& (x\_3'=0)}}
\]
This update assigns \texttt{1} to \texttt{x\_1}, \texttt{true} to \texttt{x\_2} and \texttt{0} to \texttt{x\_3}. In an update, a variable is written in primed form (with \texttt{'}) to indicate that this will be the new value of that variable. Each assignment must be enclosed in parentheses and separated with \texttt{\&} from other assignments.
The \texttt{action} can be included optionally to label the command or for synchronization purposes. Listing \ref{lst:exmpprism} shows the model file for the \mdpN \mdp from figure \ref{fig:exampleMdp}.
\begin{figure}
	\begin{lstlisting}[language=prism, caption={\prism model file for the \mdpN \mdp given by Figure \ref{fig:exampleMdp}. This example has only one variable representing the six states in \mdp and the actions \texttt{a}, \texttt{b} and \texttt{c} referring to \action, \actionb, and \actionc in \mdp, which are only used for labeling here. The initial distribution has to be realized with an additional state (\texttt{x=0}).},label={lst:exmpprism}]
		mdp
		
		module onlymodule
		
		x : [0..6] init 0;
		
		[]  x=0 -> 0.7:(x'=1) + 0.2:(x'=3) + 0.1:(x'=4);
		
		[a] x=1 -> 0.5:(x'=2) + 0.5:(x'=3);
		[b] x=1 -> (x'=4);
		[b] x=2 -> 0.6:(x'=2) + 0.4:(x'=6);
		[c] x=2 -> (x'=1);
		[b] x=3 -> 0.4:(x'=3) + 0.6:(x'=2);
		[c] x=3 -> 0.8:(x'=1) + 0.2:(x'=4);
		[a] x=4 -> 0.5:(x'=4) + 0.5:(x'=3);
		[c] x=4 -> (x'=4);
		[b] x=5 -> (x'=5);
		[a] x=6 -> (x'=6);
		
		endmodule	
	\end{lstlisting}
\end{figure}

Actions cause synchronization when included in multiple modules. For example, if \texttt{action1} is included in two commands that are in distinct modules, they will be selected at the same time. If the guard of one of the commands with \texttt{action1} is not true, none of the commands can be selected\cite{Kwiatkowska2000, Kwiatkowska2011}.


%\redcomment{not sure if I should: $I := \initdistrib$ thereby meaning the underlying set}




\end{document}
 
