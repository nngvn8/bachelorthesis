\documentclass[preview]{standalone}
%\usepackage{prelude}
\input{prelude}


\begin{document}

\section{Preliminaries} \label{ch:prelim}

%\subsection{Transition Systems}
\viewsNC will be defined on \chgphsN. Instead of directly providing the definition of \achgphN we will consider less powerful classes of models to represent systems that are extended by \chgphsN.

Firstly we will consider transition systems. Transition systems are basically digraphs consisting of \emph{states} and \emph{transitions} in place of nodes and edges. A state describes some information about a system at a certain moment of its behavior, a transition the evolution from one state to another. We will use transition systems with \emph{action names} and \emph{atomic propositions} for states as in \cite{Baier2008}.

\begin{definition}[\!{\cite{Baier2008}, Definition 2.1.}]
	A \emph{transition system} TS is a tuple \transitionsystem where
	\begin{itemize}
		\item \states is a set of states,
		\item \actions is a set of actions,
		\item $\transitionrel \subseteq \states \times \actions \times \states$ is transition relation,
		\item $\initstates \subseteq \states$ is a set of initial states,
		\item \atomicprops is a set of atomic propositions, and
		\item $\labelingfct : \states \to \powerset{\atomicprops}$
	\end{itemize}
\end{definition}

%Considering a traffic light displaying green could be considered as one state whereas displaying red could be another one. Transitions model the progression of the system from one state to another one. Sticking to the example of the traffic light a transition could model the switch from state green light being displayed to the state of red light being displayed. There are several variants of transitions systems. 



A transition system is called \emph{finite} if \states, \atomicprops and \labelingfct are finite. Actions are used for communication between processes. We denote them with Greek letters ($\alpha, \beta, \gamma, \dots$). Atomic propositions are simple facts about states. For instance "x is greater than 20" or "red and yellow light are on" could be atomic propositions. We will denote atomic propositions with arabic letters ($a,b,c,\dots$). They are assigned to a state by a labeling function \labelingfct.

The intuitive behavior of transition systems is as follows. The evolution of a transition system starts in some state $\state \in \initstates$. If the set of \initstates of initial states is empty the transition system has no behavior at all. From the initial state the transition system evolves according to the transition relation \transitionrel. The evolution ends in a state that has no outgoing transitions. For every state there may be several possible transitions to be taken. The choice of which one is take is done nondeterministically. That is the outcome of the selection can not be know a priori. It is especially not following any probability distribution. Hence there can not be made any statement about the likelihood of a transition being selected.

In contrast with Markov Chains this nondeterministic behavior is replaced with a probabilistic one. That is for every state there exists a probability distribution that describes the chance of a transition of being selected. There are no actions and no nondeterminism in Markov Chains.
%\redcomment{NOTES BEGIN}
%\begin{itemize}
%	\item Markov Chain (MC)
%	\item transition systems to markov chains: nondeterministic choices replaced by probablistic
%	\item successor chosen according to probability distribution
%	\item distribution only dependent on current state \state (not path)
%	\item system evolution not dependent on history but only current state $\to$ \emph{memoryless property}
%\end{itemize}
%
%\redcomment{NOTES END}

\begin{definition}[\!{\cite{Baier2008}, Definition 10.1.}]
	A \emph{(discrete-time) Markov chain} is a tuple $\autm = (\states, \probtfunc, \initdistrib, \atomicprops, \labelingfct)$ where 
	\begin{itemize}
		\item \states is a countable, nonempty set of states,
		\item $\probtfunc : \states \times \states \to [0,1]$ is the \emph{transition probability function}, such that for all states \state:
		\begin{center}
			$\bbigsum{\state' \in \states}\probtfunc(\state,\states') = 1$.	
		\end{center}
		\item $\initdistrib : \states \to [0,1]$ is the \emph{initial distribution}, such that $\mathlarger{\sum_{\state \in \states}} \initdistrib(\state) = 1$, and
		\item \atomicprops is a set of atomic propositions and,
		\item $\labelingfct : \states \to \powerset{\atomicprops}$ a labeling function.		
	\end{itemize}
\end{definition}

A Markov Chain (MC) \autm is called \emph{finite} if \states and \atomicprops are finite. \redcomment{For finite \autm, the \emph{size} of \autm, denoted \emph{size}(\autm),  is the number of states plus the number of pairs $(\state, \state') \in \states \times \states$ with $\probtfunc(\state, \state') > 0.$ OMIT??}

%It is important to note that Markov Chains are memoryless in the sense that the system evolution is not dependent on the history of only on the current state. That is the evolution of the system does not depend on the sequence of so far traversed states with the transitions. 

The probability function \probtfunc specifies for each state \state the probability $\probtfunc(\state,\state')$ of moving from \state to \state' in one step. The constraint put on \probtfunc in the second item ensures that \probtfunc is a probability distribution. The value $\initdistrib(\state)$ specifies the likelihood that the system evolution starts in \state. All states \state with $\initdistrib(\state) > 0$ are  considered \emph{initial states}. States \state' with $\probtfunc(\state, \state') > 0$ are viewed as possible successors of the state \state. The operational behavior is as follows. A initial state $\state_0$ with $\initdistrib(\state_0) > 0$ is yielded. Afterwards in each state a transition is yielded at random according to the probability distribution \probtfunc in that state. \redcomment{The evolution of a Markov chain ends in a state \state \iffN $\probtfunc(\state, \state) = 1$}.
%\begin{itemize}
%	\redcomment{\item has no actions \\ "As compositional approaches for Markov models are outside the scope of this monograph,
%	actions are irrelevant in this chapter and are therefore omitted."}
%\end{itemize}

%\redcomment{NOTES BEGIN}
%\begin{itemize}
%	\item Probability Function \probtfunc specifies for each state \state the probability \probtfunc(\state,\state') of moving from \state to \state' in one step.
%	\item constraint on \probtfunc ensures that \probtfunc is distribution
%	\item \initdistrib(\state) specifies system evolution starts in \state
%	\item states \state with $\initdistrib(\state) > 0$ are  considered \emph{initial states}
%	\item states \state' with $\probtfunc(\state, \state') > 0$ are view as possible successors of \state
%	\item has no actions \\ "As compositional approaches for Markov models are outside the scope of this monograph,
%	actions are irrelevant in this chapter and are therefore omitted."
%\end{itemize}
%
%\redcomment{NOTES END}


%\subsection{Markov Decision Process} 

%\redcomment{NOTES BEGIN}
The disadvantage of Markov Chains is that they do not enable process intercommunication and do not permit nondeterminism, but only probabilistic evolution. Markov Decision Processes (\mdpsN) permit both probabilistic and nondeterministic choices. An \mdpN thereby is a model that somewhat merges the concept of transition systems with the concept of Markov chains. 
%\begin{itemize}
%	\item probabilistic choices: possible outcomes for of randomized actions -> requires statistical experiments to obtain adequate distributions that model average behavior of the environment
%	\item information not available or guarantee about system properties is required -> nondeterminism
%	\item Another example: randomized distributed algorithms. Non-determinism: interleaving behavior: nondeterministic choice which process, probabilistic: have rather restricted set of actions that have a random nature
%	\item used for abstraction in markov chains: states grouped by \atomicprops and have a wide range of transition probabilities -> essentially nondeterminism -> transition probabilities are replaced by nondeterminism		
%\end{itemize}
%
%\redcomment{NOTES END}
%\redcomment{NOTES BEGIN}
%
%\begin{itemize}
%	\item Markov decision process (MDP) 
%	\item idea: Adding nondeterminism to markov chains. MDPs permit both probabilistic and nondeterministic choices 
%	\item probabilistic choices: possible outcomes for of randomized actions -> requires statistical experiments to obtain adequate distributions that model average behavior of the environment
%	\item information not available or guarantee about system properties is required -> nondeterminism
%	\item Another example: randomized distributed algorithms. Non-determinism: interleaving behavior: nondeterministic choice which process, probabilistic: have rather restricted set of actions that have a random nature
%	\item used for abstraction in markov chains: states grouped by \atomicprops and have a wide range of transition probabilities -> essentially nondeterminism -> transition probabilities are replaced by nondeterminism		
%\end{itemize}

%\redcomment{NOTES END}

\begin{definition}[\!{\cite{Baier2008}, Definition 10.81.}]
	A \emph{Markov decision process} (\mdpN) is a tuple $\mdp = \mdptuple$ where
	\begin{itemize}
		\item \states is a countable set of states,
		\item \actions is a set of actions,
		\item $\probtfunc : \states \times \actions \times \states \to [0,1]$ is the transition probability function such that for all states $\state \in \states$ and actions $\action \in \actions$:
		\begin{center}
			$\bbigsum{\state' \in \states}\probtfunc(\state,\action,\state') \in \{0,1\}$,
		\end{center}
		\item $\initdistrib : \states \to [0,1]$ is the initial distribution such that $\sum_{\state \in \states} \initdistrib(\state) = 1$,
		\item \atomicprops is  a set of atomic propositions and
		\item $\labelingfct : \states \to \powerset{\atomicprops}$ a labeling function.
	\end{itemize}
	An action \action is \emph{enabled} in state \state if and only if $\sum_{\state' \in \states} \probtfunc(\state,\action,\state') = 1$. Let $\actions(\state)$ denote the set of enabled actions in \state. For any state $\state \in \states$, it is required that $\actions(\state) \neq \emptyset$. Each state \state' for which $\probtfunc(\state,\action,\state') > 0$ is called an \emph{\action-successor} of \state.
\end{definition}

%It adds probabilistic choices to transition systems or add nondeterminism to Markov chains. Probabilistic choices may be used to model the (probabilistic) behavior of the environment. But to do so statistical experiments are required to obtain adequate distributions that model the average behavior of the environment. If this information is not available too hard to obtain, or a guarantee about system properties is required nondeterminism is the natural option for modeling in these cases. Further usecases of \mdpsN are randomized distributed algorithms and abstraction in Markov chains. Randomized distributed algorithms are nondeterministic because there is a nondeterministic choice which process performs the next step and randomized because they have rather restricted set of actions that have a random nature. Abstraction in Markov chains can be feasible if states have been grouped by \atomicprops and and have a wide range of transition probabilities. If that is the case selection of a transition is rather non deterministic and thereby can be abstracted with an \mdpN by replacing the probabilities with nondeterminism.

An \mdpN is called \emph{finite} if \states, \actions and \atomicprops are finite.The transition probabilities $\probtfunc(\state, \action, t)$ can be arbitrary real numbers in $[0,1]$ (that sum up to 1 or 0 for fixed \state and \action). For algorithmic purposes they are assumed to be rational.  The unique initial distribution \initdistrib could be generalized to set of \initdistrib with nondeterministic choice at the beginning. For sake of simplicity there is just one single distribution. The operational behavior is as follows. A starting state $\state_0$ yielded by \initdistrib with $\initdistrib(\state_0) > 0$. From there on nondeterministic choice of enabled action takes place followed by a probabilistic choice of a state. The action fixed in the step of nondeterministic selection. Any Markov chain is an \mdpN in which for every state \state, $\actions(\state)$ is a singleton set. Conversely an \mdpN with the property of $\outacts(\state) = 1$ is a Markov chain. Thus Markov chains are a proper subset of Markov decision processes.

We define $\outacts(\state) := \{\action \mid (\state,\action,\smstate) \in \trans\}$ for $\state \in \states$. For $\state \in \states$ we call an element of $\outacts(\state)$ outgoing action of \state. We use analogous definition and terminology for incoming actions \inacts. For convenience for $\state_1, \state_2 \in \states$ and $\action \in \actions$ we will write $(\state_1, \action, \state_2) \in \probtfunc$ \iffN $\probtfunc(\state_1, \action, \state_2) > 0$, that is to say if there is a non-zero probability of evolving from state $\state_1$ to state $\state_2$ with action \action. Analogously we will write $\state \in \initdistrib$ \iffN is an initial state ($\initdistrib(\state) > 0$). 

\begin{exmp}
In Figure \ref{fig:exampleMdp} we see an graphical representation of an \mdpN. For this \mdpN it is $\states = \{\state_1, \state_2, \state_3, \state_4, \state_5, \state_6\}$, $\actions = \{\action, \actionb, \actionc\}$, $\atomicprops = \{a,b\}$. In Table \ref{tab:atomicpropsandlabelingfunction} we see the values for \labelingfct, \initdistrib and \trans.

\begin{table}[h!]
	\parbox{.45\linewidth}{
		\begin{center}
%			
			\begin{tabular}{c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
				State & $\labelingfct(\state_i)$ & $\initdistrib(\state_i)$\\		
				\hline
				$\state_1$ & $\emptyset$ & $0.7$\\
				$\state_2$ & $\{a\}$ & $0$\\
				$\state_3$ & $\{a,b\}$ & $0.2$\\
				$\state_4$ & $\{b\}$ & $0.1$\\
				$\state_5$ & $\{a\}$ & $0$\\
				$\state_6$ & $\{b\}$ & $0$\\			
			\end{tabular}
		\end{center}
}
%\hspace{10mm}
\parbox{.45\linewidth}{
		\begin{center}
%			\caption{Your first table.}
%			\label{tab:trans}
			\begin{tabular}{c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
				$t \in \states \times \actions \times \states$ & $\trans(t)$\\		
				\hline
				$(\state_1,\action, \state_2)$ & $0.5$\\
				$(\state_1,\action, \state_4)$ & $1$\\
				$(\state_2,\actionb, \state_2)$ & $0.6$\\
				$(\state_2,\actionb, \state_6)$ & $0.4$\\
				$(\state_2,\actionc, \state_1)$ & $1$\\
				$(\state_3,\actionb, \state_3)$ & $0.4$\\
				$(\state_3,\actionb, \state_2)$ & $0.6$\\
				$(\state_3,\actionc, \state_1)$ & $0.8$\\
				$(\state_3,\actionc, \state_4)$ & $0.2$\\
				$(\state_4,\action, \state_4)$ & $0.5$\\
				$(\state_4,\actionc, \state_4)$ & $1$\\
				$(\state_4,\action, \state_3)$ & $0.5$\\
				$(\state_5,\actionb, \state_5)$ & $1$\\
				$(\state_6,\action, \state_6)$ & $1$\\				
			\end{tabular}
		\end{center}
	}
	\caption{Labeling Function, initial distribution and transition probability function of \mdpN in Figure \ref{fig:exampleMdp}. Omitted values of $\trans(t)$ are meant to be zero.}
	\label{tab:atomicpropsandlabelingfunction}
\end{table}

We will declare \mdpsN by only providing its graphical representation. For this the sets $\states, \actions, \atomicprops$ are assumed to be minimal. That is, they contain no more elements than displayed in the graphical representation. Mostly we will use simplified graphical representations of \mdpsN. In these we will omit information. If actions are omitted, it is assumed that each transition $t \in \trans$ has a distinct action. If probabilities are omitted for each state it is assumed the uniform distribution on each set of outgoing transitions with the same action. If the initial distribution is omitted, it is assumed to be the uniform distribution. If atomic propositions are omitted it is assumed that the set of atomic propositions is empty and every state is mapped to the empty set by the labeling function. The purpose of simplified representations is to focus on and only show relevant information. The remaining information is considered irrelevant in these cases.

\begin{figure}[!htb]
	\centering \input{./02/images/exampleMdp}
	\caption{Simplified representation of \mdp (left) and the \viewN \viewinitstates on it(left)}
	\label{fig:exampleMdp}  
\end{figure}
\end{exmp}

In the implementation and evaluation we will refer to \prism (PRobabilistIc Symbolic Model checker), which is why we will give a brief introduction on it. \prism is a model checker that can be used to define models and check them in an automated manner. In \prism a model is defined with modules, which interact with each other. A module consists of variables and commands. The (current) values of all the variables in the module define the (current) local state of the module. The global state of the model is defined by the local state of all modules.
%	\item Let in the following $x_1, \dots, x_l$ be variables of a module and $n_i$ the (current) value of $x_i$. The state of a module is defined by the value of its variables $(n_1, \dots, n_l)$
%	\item Let $\{y_1, \dots y_m\} \subseteq \{x_1, \dots, x_l\}$
A command in a module is of the form 
\[
\text{\texttt{[action] guard -> p\_1\!\;:\!\;update\_1\space+\space\dots\;\,+\space p\_m\!\;:\!\;update\_m;}} 
\]
where \texttt{p\_1, \dots\;\,, p\_m > 0} and \texttt{p\_1 + \dots\;\,+ p\_m = 1}.
The \texttt{guard} is a predicate on all variables of the model (including those of other modules). It may include operators such as negation (\texttt{!}), conjunction (\texttt{\&}) disjunction (\texttt{|}), arithmetic operators (\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}) or relational operators (\texttt{<}, \texttt{<=}, \texttt{>=}, \texttt{>}, \texttt{!=}, \texttt{=}, \texttt{=>}, \texttt{<=>}) as well as predefined functions.
An \texttt{update} represents a transition in the model which normally reflects a change of state. As a state is defined by the value of all the variables an update is specified by the assignment of new values to the variables of the module, possibly as a function of variables from other modules. The value of a variable remains unchanged, if it is not assigned a new value in an update. An update could look as follows:
\[
\text{\texttt{(x\_1'=1) \& (x\_2'=true) \& (x\_3'=0)}}
\]
It assigns \texttt{1} to \texttt{x\_1}, \texttt{true} to \texttt{x\_2} and \texttt{0} to \texttt{x\_3}. In an update a variable is written in primed form (with ') to indicate that this will be the new value of that variable. Each assignment has to be in parentheses and separated with \texttt{\&} from other assignments.
The \texttt{action} can be included optionally for labeling the command or for synchronization purposes. 
\begin{figure}
	\begin{lstlisting}[language=prism, caption={\prism model file for the \mdpN \mdp given by Figure \ref{fig:exampleMdp}. This example has only one variable representing the six states in \mdp and the actions \texttt{a}, \texttt{b} and \texttt{c} referring to \action, \actionb, and \actionc in \mdp, which are only used for labeling here.},label={lst:exmpprism}]
		mdp
		
		module onlymodule
		
		x : [1..6];
		
		[a] x=1 -> 0.5:(x'=2) + 0.5:(x'=3);
		[b] x=1 -> (x'=4);
		[b] x=2 -> 0.6:(x'=2) + 0.4:(x'=6);
		[c] x=2 -> (x'=1);
		[b] x=3 -> 0.4:(x'=3) + 0.6:(x'=2);
		[c] x=3 -> 0.8:(x'=1) + 0.2:(x'=4);
		[a] x=4 -> 0.5:(x'=4) + 0.5:(x'=3);
		[c] x=4 -> (x'=4);
		[b] x=5 -> (x'=5);
		[a] x=6 -> (x'=6);
		
		endmodule
		
		
		init
			x=1 | x=3 | x=4
		endinit		
	\end{lstlisting}
\end{figure}

Actions cause synchronization when included in several modules. If for example \texttt{action1} is included in two commands in distinct modules these will be chosen simultaneously. If the guard of one of the commands with \texttt{action1} is not true neither of the commands can be chosen\cite{Kwiatkowska2000, Kwiatkowska2011}.


%\redcomment{not sure if I should: $I := \initdistrib$ thereby meaning the underlying set}




\end{document}
 
